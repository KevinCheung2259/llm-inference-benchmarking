#!/usr/bin/env python3
"""
æµ‹è¯•ä¿®æ”¹åçš„è„±æ•å·¥å…·JSONLè¾“å‡ºåŠŸèƒ½
"""

import json
import tempfile
import os
import sys
import subprocess

def create_sample_log_file():
    """åˆ›å»ºç¤ºä¾‹æ—¥å¿—æ–‡ä»¶"""
    sample_lines = [
        '2024-01-01T10:00:01Z {"message": "[Log chat request] {\\"conversationId\\": \\"user_12345_session_abc\\", \\"body\\": {\\"prompt\\": [{\\"role\\": \\"user\\", \\"content\\": \\"ä½ å¥½ï¼Œè¯·å¸®æˆ‘å†™ä¸€ä¸ªPythonç¨‹åº\\"}]}}"}',
        '2024-01-01T10:00:02Z {"message": "[Log chat request] {\\"conversationId\\": \\"user_67890_session_def\\", \\"body\\": {\\"prompt\\": [{\\"role\\": \\"user\\", \\"content\\": \\"Hello, can you help me with machine learning?\\"}]}}"}',
        '2024-01-01T10:00:03Z {"message": "[Log chat request] {\\"conversationId\\": \\"user_12345_session_abc\\", \\"body\\": {\\"prompt\\": [{\\"role\\": \\"user\\", \\"content\\": \\"è°¢è°¢ï¼Œè¿™ä¸ªå‡½æ•°çœ‹èµ·æ¥ä¸é”™\\"}]}}"}'
    ]
    
    return sample_lines

def test_desensitize_jsonl():
    """æµ‹è¯•è„±æ•å·¥å…·çš„JSONLè¾“å‡ºåŠŸèƒ½"""
    print("ğŸ§ª å¼€å§‹æµ‹è¯•è„±æ•å·¥å…·JSONLè¾“å‡ºåŠŸèƒ½...")
    
    # åˆ›å»ºç¤ºä¾‹æ—¥å¿—æ–‡ä»¶
    sample_lines = create_sample_log_file()
    print(f"âœ… åˆ›å»ºäº† {len(sample_lines)} è¡Œç¤ºä¾‹æ—¥å¿—")
    
    # åˆ›å»ºä¸´æ—¶æ–‡ä»¶
    with tempfile.NamedTemporaryFile(mode='w', suffix='.log', delete=False, encoding='utf-8') as temp_log:
        for line in sample_lines:
            temp_log.write(line + '\n')
        temp_log_path = temp_log.name
    
    temp_jsonl_path = temp_log_path.replace('.log', '_desensitized.jsonl')
    temp_mapping_path = temp_log_path.replace('.log', '_mappings.json')
    
    try:
        print(f"ğŸ“ ä¸´æ—¶æ—¥å¿—æ–‡ä»¶: {temp_log_path}")
        print(f"ğŸ“ è¾“å‡ºJSONLæ–‡ä»¶: {temp_jsonl_path}")
        print(f"ğŸ“ æ˜ å°„æ–‡ä»¶: {temp_mapping_path}")
        
        # è¿è¡Œè„±æ•å·¥å…·ï¼ˆä½¿ç”¨ç®€å•çš„tokenizeré¿å…ç½‘ç»œé—®é¢˜ï¼‰
        print("ğŸ”„ è¿è¡Œè„±æ•å·¥å…·...")
        
        # æ£€æŸ¥è„šæœ¬æ˜¯å¦å­˜åœ¨
        script_path = "replay_logs_desensitize/desensitize_dataset_tokenizer.py"
        if not os.path.exists(script_path):
            print(f"âŒ è„šæœ¬æ–‡ä»¶ä¸å­˜åœ¨: {script_path}")
            return
        
        # è¿è¡Œè„šæœ¬ï¼ˆä½¿ç”¨gpt2ä½œä¸ºå¤‡ç”¨tokenizerï¼‰
        cmd = [
            sys.executable, script_path,
            "--input", temp_log_path,
            "--output", temp_jsonl_path,
            "--mapping", temp_mapping_path,
            "--model-name", "gpt2",  # ä½¿ç”¨è¾ƒå°çš„æ¨¡å‹é¿å…ç½‘ç»œé—®é¢˜
            "--verbose"
        ]
        
        try:
            result = subprocess.run(cmd, capture_output=True, text=True, timeout=120)
            if result.returncode != 0:
                print(f"âŒ è„±æ•å·¥å…·è¿è¡Œå¤±è´¥:")
                print(f"STDOUT: {result.stdout}")
                print(f"STDERR: {result.stderr}")
                return
            else:
                print("âœ… è„±æ•å·¥å…·è¿è¡ŒæˆåŠŸ")
        except subprocess.TimeoutExpired:
            print("âŒ è„±æ•å·¥å…·è¿è¡Œè¶…æ—¶")
            return
        except Exception as e:
            print(f"âŒ è¿è¡Œè„±æ•å·¥å…·æ—¶å‡ºé”™: {e}")
            return
        
        # éªŒè¯è¾“å‡ºæ–‡ä»¶
        if not os.path.exists(temp_jsonl_path):
            print("âŒ JSONLæ–‡ä»¶æœªç”Ÿæˆ")
            return
        
        print("ğŸ” éªŒè¯JSONLæ–‡ä»¶æ ¼å¼...")
        
        # è¯»å–JSONLæ–‡ä»¶å¹¶éªŒè¯æ ¼å¼
        jsonl_items = []
        with open(temp_jsonl_path, 'r', encoding='utf-8') as f:
            for line_num, line in enumerate(f, 1):
                try:
                    item = json.loads(line.strip())
                    jsonl_items.append(item)
                    
                    # éªŒè¯å¿…è¦å­—æ®µ
                    if 'timestamp' not in item:
                        print(f"âŒ ç¬¬{line_num}è¡Œç¼ºå°‘timestampå­—æ®µ")
                        return
                    if 'conversation_id' not in item:
                        print(f"âŒ ç¬¬{line_num}è¡Œç¼ºå°‘conversation_idå­—æ®µ")
                        return
                    if 'messages' not in item:
                        print(f"âŒ ç¬¬{line_num}è¡Œç¼ºå°‘messageså­—æ®µ")
                        return
                    
                    # éªŒè¯æ²¡æœ‰metadataå­—æ®µ
                    if 'metadata' in item:
                        print(f"âŒ ç¬¬{line_num}è¡ŒåŒ…å«ä¸åº”å­˜åœ¨çš„metadataå­—æ®µ")
                        return
                    
                except json.JSONDecodeError as e:
                    print(f"âŒ ç¬¬{line_num}è¡ŒJSONè§£æå¤±è´¥: {e}")
                    return
        
        print(f"âœ… JSONLæ ¼å¼éªŒè¯é€šè¿‡ï¼Œå…± {len(jsonl_items)} è¡Œ")
        
        # æ˜¾ç¤ºJSONLæ–‡ä»¶å†…å®¹ç¤ºä¾‹
        print("\nğŸ“‹ JSONLæ–‡ä»¶å†…å®¹ç¤ºä¾‹:")
        for i, item in enumerate(jsonl_items[:2], 1):
            print(f"ç¬¬{i}è¡Œ:")
            print(f"  - timestamp: {item['timestamp']}")
            print(f"  - conversation_id: {item['conversation_id']}")
            print(f"  - messagesæ•°é‡: {len(item['messages'])}")
            if item['messages']:
                first_msg = item['messages'][0]
                if 'content' in first_msg and isinstance(first_msg['content'], list):
                    print(f"  - ç¬¬ä¸€ä¸ªæ¶ˆæ¯tokenæ•°é‡: {len(first_msg['content'])}")
                    print(f"  - å‰5ä¸ªtoken IDs: {first_msg['content'][:5]}")
        
        # éªŒè¯æ˜ å°„æ–‡ä»¶
        if os.path.exists(temp_mapping_path):
            print("\nğŸ” éªŒè¯æ˜ å°„æ–‡ä»¶...")
            with open(temp_mapping_path, 'r', encoding='utf-8') as f:
                mappings = json.load(f)
            
            required_fields = ['model_name', 'tokenizer_vocab_size', 'shuffle_table', 'conversation_id_mapping', 'statistics']
            for field in required_fields:
                if field not in mappings:
                    print(f"âŒ æ˜ å°„æ–‡ä»¶ç¼ºå°‘{field}å­—æ®µ")
                    return
            
            print("âœ… æ˜ å°„æ–‡ä»¶æ ¼å¼æ­£ç¡®")
            print(f"  - æ¨¡å‹åç§°: {mappings['model_name']}")
            print(f"  - è¯æ±‡è¡¨å¤§å°: {mappings['tokenizer_vocab_size']}")
            print(f"  - ç½®æ¢è¡¨å¤§å°: {mappings['statistics']['shuffle_table_size']}")
            print(f"  - å¤„ç†çš„ä¼šè¯æ•°: {mappings['statistics']['processed_conversations']}")
        
        # æ£€æŸ¥æ–‡ä»¶å¤§å°
        jsonl_size = os.path.getsize(temp_jsonl_path)
        print(f"\nğŸ“Š æ–‡ä»¶ç»Ÿè®¡:")
        print(f"  - JSONLæ–‡ä»¶å¤§å°: {jsonl_size} å­—èŠ‚")
        print(f"  - å¹³å‡æ¯è¡Œå¤§å°: {jsonl_size / len(jsonl_items):.1f} å­—èŠ‚")
        
        print("\nâœ… æ‰€æœ‰æµ‹è¯•é€šè¿‡! è„±æ•å·¥å…·JSONLè¾“å‡ºåŠŸèƒ½æ­£å¸¸")
        
    finally:
        # æ¸…ç†ä¸´æ—¶æ–‡ä»¶
        for path in [temp_log_path, temp_jsonl_path, temp_mapping_path]:
            if os.path.exists(path):
                os.unlink(path)
        print("ğŸ—‘ï¸ ä¸´æ—¶æ–‡ä»¶æ¸…ç†å®Œæˆ")

if __name__ == "__main__":
    test_desensitize_jsonl() 