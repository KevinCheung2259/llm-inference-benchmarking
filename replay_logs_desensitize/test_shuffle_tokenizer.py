#!/usr/bin/env python3
"""
æµ‹è¯•ç½®æ¢è¡¨è„±æ•åŠŸèƒ½çš„è„šæœ¬
ä½¿ç”¨ç®€å•çš„å­—ç¬¦tokenizeré¿å…ç½‘ç»œä¾èµ–
"""

import json
import tempfile
import os
import random
from datetime import datetime

class SimpleTokenizer:
    """ç®€å•çš„å­—ç¬¦çº§tokenizerï¼Œç”¨äºæµ‹è¯•"""
    
    def __init__(self, vocab_size=1000):
        # åˆ›å»ºç®€å•çš„è¯æ±‡è¡¨ï¼šå¸¸ç”¨å­—ç¬¦ + æ•°å­— + è‹±æ–‡å­—æ¯ + ä¸­æ–‡å­—ç¬¦
        vocab = []
        
        # æ·»åŠ ç‰¹æ®Štoken
        special_tokens = ['<pad>', '<unk>', '<start>', '<end>']
        vocab.extend(special_tokens)
        
        # æ·»åŠ åŸºæœ¬å­—ç¬¦
        for i in range(32, 127):  # åŸºæœ¬ASCIIå­—ç¬¦
            vocab.append(chr(i))
        
        # æ·»åŠ ä¸€äº›å¸¸ç”¨ä¸­æ–‡å­—ç¬¦
        common_chinese = "çš„ä¸€æ˜¯åœ¨ä¸äº†æœ‰å’Œäººè¿™ä¸­å¤§ä¸ºä¸Šä¸ªå›½æˆ‘ä»¥è¦ä»–æ—¶æ¥ç”¨ä»¬ç”Ÿåˆ°ä½œåœ°äºå‡ºå°±åˆ†å¯¹æˆä¼šå¯ä¸»å‘å¹´åŠ¨åŒå·¥ä¹Ÿèƒ½ä¸‹è¿‡å­è¯´äº§ç§é¢è€Œæ–¹åå¤šå®šè¡Œå­¦æ³•æ‰€æ°‘å¾—ç»åä¸‰ä¹‹è¿›ç€ç­‰éƒ¨åº¦å®¶ç”µåŠ›é‡Œå¦‚æ°´åŒ–é«˜è‡ªäºŒç†èµ·å°ç‰©ç°å®åŠ é‡éƒ½ä¸¤ä½“åˆ¶æœºå½“ä½¿ç‚¹ä»ä¸šæœ¬å»æŠŠæ€§å¥½åº”å¼€å®ƒåˆè¿˜å› ç”±å…¶äº›ç„¶å‰å¤–å¤©æ”¿å››æ—¥é‚£ç¤¾ä¹‰äº‹å¹³å½¢ç›¸å…¨è¡¨é—´æ ·ä¸å…³å„é‡æ–°çº¿å†…æ•°æ­£å¿ƒåä½ æ˜çœ‹åŸåˆä¹ˆåˆ©æ¯”æˆ–ä½†è´¨æ°”ç¬¬å‘é“å‘½æ­¤å˜æ¡åªæ²¡ç»“è§£é—®æ„å»ºæœˆå…¬æ— ç³»å†›å¾ˆæƒ…è€…æœ€ç«‹ä»£æƒ³å·²é€šå¹¶æç›´é¢˜å…šç¨‹å±•äº”æœæ–™è±¡å‘˜é©ä½å…¥å¸¸æ–‡æ€»æ¬¡å“å¼æ´»è®¾åŠç®¡ç‰¹ä»¶é•¿æ±‚è€å¤´åŸºèµ„è¾¹æµè·¯çº§å°‘å›¾å±±ç»Ÿæ¥çŸ¥è¾ƒå°†ç»„è§è®¡åˆ«å¥¹æ‰‹è§’æœŸæ ¹è®ºè¿å†œæŒ‡å‡ ä¹åŒºå¼ºæ”¾å†³è¥¿è¢«å¹²åšå¿…æˆ˜å…ˆå›åˆ™ä»»å–æ®å¤„é˜Ÿå—ç»™è‰²å…‰é—¨å³ä¿æ²»åŒ—é€ ç™¾è§„çƒ­é¢†ä¸ƒæµ·å£ä¸œå¯¼å™¨å‹å¿—ä¸–é‡‘å¢äº‰æµé˜¶æ²¹æ€æœ¯æäº¤å—è”ä»€è®¤å…­å…±æƒæ”¶è¯æ”¹æ¸…å·±ç¾å†é‡‡è½¬å•é£åˆ‡æ‰“ç™½æ•™é€ŸèŠ±å¸¦å®‰åœºèº«è½¦ä¾‹çœŸåŠ¡å…·ä¸‡æ¯ç›®è‡³è¾¾èµ°ç§¯ç¤ºè®®å£°æŠ¥æ–—å®Œç±»å…«ç¦»ååç¡®æ‰ç§‘å¼ ä¿¡é©¬èŠ‚è¯ç±³æ•´ç©ºå…ƒå†µä»Šé›†æ¸©ä¼ åœŸè®¸æ­¥ç¾¤å¹¿çŸ³è®°éœ€æ®µç ”ç•Œæ‹‰æ—å¾‹å«ä¸”ç©¶è§‚è¶Šç»‡è£…å½±ç®—ä½æŒéŸ³ä¼—ä¹¦å¸ƒå¤å®¹å„¿é¡»é™…å•†ééªŒè¿æ–­æ·±éš¾è¿‘çŸ¿åƒå‘¨å§”ç´ æŠ€å¤‡åŠåŠé’çœåˆ—ä¹ å“çº¦æ”¯èˆ¬å²æ„ŸåŠ³ä¾¿å›¢å¾€é…¸å†å¸‚å…‹ä½•é™¤æ¶ˆæ„åºœç§°å¤ªå‡†ç²¾å€¼å·ç‡æ—ç»´åˆ’é€‰æ ‡å†™å­˜å€™æ¯›äº²å¿«æ•ˆæ–¯é™¢æŸ¥æ±Ÿå‹çœ¼ç‹æŒ‰æ ¼å…»æ˜“ç½®æ´¾å±‚ç‰‡å§‹å´ä¸“çŠ¶è‚²å‚äº¬è¯†é€‚å±åœ†åŒ…ç«ä½è°ƒæ»¡å¿å±€ç…§å‚çº¢ç»†å¼•å¬è¯¥é“ä»·ä¸¥"
        for char in common_chinese:
            if char not in vocab:
                vocab.append(char)
        
        # è¡¥å……åˆ°æŒ‡å®šå¤§å°
        while len(vocab) < vocab_size:
            vocab.append(f"<unk_{len(vocab)}>")
        
        self.vocab = vocab[:vocab_size]
        self.vocab_size = len(self.vocab)
        self.char_to_id = {char: i for i, char in enumerate(self.vocab)}
        self.id_to_char = {i: char for i, char in enumerate(self.vocab)}
    
    def encode(self, text, add_special_tokens=False):
        """å°†æ–‡æœ¬ç¼–ç ä¸ºtoken IDs"""
        token_ids = []
        if add_special_tokens:
            token_ids.append(self.char_to_id.get('<start>', 2))
        
        for char in text:
            token_id = self.char_to_id.get(char, self.char_to_id.get('<unk>', 1))
            token_ids.append(token_id)
        
        if add_special_tokens:
            token_ids.append(self.char_to_id.get('<end>', 3))
        
        return token_ids
    
    def decode(self, token_ids, skip_special_tokens=False):
        """å°†token IDsè§£ç ä¸ºæ–‡æœ¬"""
        chars = []
        special_tokens = {'<pad>', '<unk>', '<start>', '<end>'}
        
        for token_id in token_ids:
            if token_id < self.vocab_size:
                char = self.id_to_char[token_id]
                if skip_special_tokens and char in special_tokens:
                    continue
                # è·³è¿‡ç”Ÿæˆçš„unk token
                if char.startswith('<unk_'):
                    continue
                chars.append(char)
        
        return ''.join(chars)
    
    def __len__(self):
        return self.vocab_size


class SimpleTokenizerBasedDesensitizer:
    """ä½¿ç”¨ç®€å•tokenizerçš„è„±æ•å¤„ç†å™¨"""
    
    def __init__(self, vocab_size=1000):
        self.tokenizer = SimpleTokenizer(vocab_size)
        self.vocab_size = len(self.tokenizer)
        
        # æ˜ å°„å…³ç³»
        self.token_id_mapping = {}
        self.conversation_id_mapping = {}
        
        # ç»Ÿè®¡ä¿¡æ¯
        self.processed_lines = 0
        self.skipped_lines = 0
        self.processed_conversations = set()
        
        # åˆ›å»ºtoken IDçš„ç½®æ¢è¡¨
        self._create_shuffle_table()
    
    def _create_shuffle_table(self):
        """åˆ›å»ºtoken IDç½®æ¢è¡¨"""
        print("æ­£åœ¨åˆ›å»ºtoken IDç½®æ¢è¡¨...")
        
        # ç”Ÿæˆä¸åŸè¯è¡¨ç­‰é•¿çš„ç½®æ¢è¡¨
        original_ids = list(range(self.vocab_size))
        shuffled_ids = list(range(self.vocab_size))
        
        # ä½¿ç”¨å›ºå®šç§å­ç¡®ä¿å¯å¤ç°æ€§
        random.seed(42)
        random.shuffle(shuffled_ids)
        
        # åˆ›å»ºä¸€å¯¹ä¸€çš„æ˜ å°„å…³ç³»ï¼ˆç½®æ¢è¡¨ï¼‰
        for i, original_id in enumerate(original_ids):
            self.token_id_mapping[original_id] = shuffled_ids[i]
        
        print(f"åˆ›å»ºäº† {len(self.token_id_mapping)} ä¸ªtoken IDæ˜ å°„çš„ç½®æ¢è¡¨")
    
    def _desensitize_text(self, text):
        """å¯¹æ–‡æœ¬è¿›è¡Œtokenizeråˆ†è¯å¹¶è¿”å›è„±æ•åçš„token IDs"""
        if not text:
            return []
        
        # ä½¿ç”¨tokenizerè¿›è¡Œåˆ†è¯
        token_ids = self.tokenizer.encode(text, add_special_tokens=False)
        
        # ä½¿ç”¨ç½®æ¢è¡¨å¯¹token IDsè¿›è¡Œè„±æ•æ˜ å°„
        desensitized_ids = []
        for token_id in token_ids:
            if token_id < self.vocab_size:
                desensitized_ids.append(self.token_id_mapping[token_id])
            else:
                print(f"è­¦å‘Š: Token ID {token_id} è¶…å‡ºè¯æ±‡è¡¨èŒƒå›´ {self.vocab_size}")
                desensitized_ids.append(token_id % self.vocab_size)
        
        return desensitized_ids
    
    def _desensitize_conversation_id(self, conversation_id):
        """å¯¹ä¼šè¯IDè¿›è¡Œè„±æ•"""
        if conversation_id in self.conversation_id_mapping:
            return self.conversation_id_mapping[conversation_id]
        
        # ç”Ÿæˆæ–°çš„ä¼šè¯ID
        import hashlib
        hash_obj = hashlib.md5(conversation_id.encode('utf-8'))
        hex_hash = hash_obj.hexdigest()
        new_id = f"conv_{hex_hash[:8]}_{hex_hash[8:16]}"
        self.conversation_id_mapping[conversation_id] = new_id
        
        return new_id
    
    def decode_tokens(self, token_ids):
        """å°†è„±æ•åçš„token IDsè§£ç ä¸ºæ–‡æœ¬"""
        # åˆ›å»ºåå‘ç½®æ¢è¡¨
        if not hasattr(self, '_reverse_mapping'):
            self._reverse_mapping = {v: k for k, v in self.token_id_mapping.items()}
        
        # åå‘æ˜ å°„token IDsåˆ°åŸå§‹IDs
        original_ids = []
        for token_id in token_ids:
            if token_id in self._reverse_mapping:
                original_ids.append(self._reverse_mapping[token_id])
            else:
                original_ids.append(token_id)
        
        return self.tokenizer.decode(original_ids, skip_special_tokens=True)


def create_sample_log_data():
    """åˆ›å»ºç¤ºä¾‹æ—¥å¿—æ•°æ®"""
    sample_data = [
        {
            "timestamp": "2024-01-01T10:00:01Z",
            "conversationId": "user_12345_session_abc",
            "body": {
                "prompt": [
                    {"role": "user", "content": "ä½ å¥½ï¼Œè¯·å¸®æˆ‘å†™ä¸€ä¸ªPythonç¨‹åº"},
                    {"role": "assistant", "content": "å¥½çš„ï¼Œæˆ‘å¯ä»¥å¸®æ‚¨å†™Pythonç¨‹åºã€‚è¯·å‘Šè¯‰æˆ‘æ‚¨å…·ä½“éœ€è¦ä»€ä¹ˆåŠŸèƒ½ï¼Ÿ"},
                    {"role": "user", "content": "æˆ‘éœ€è¦ä¸€ä¸ªè®¡ç®—æ–æ³¢é‚£å¥‘æ•°åˆ—çš„å‡½æ•°"}
                ]
            }
        },
        {
            "timestamp": "2024-01-01T10:00:02Z", 
            "conversationId": "user_67890_session_def",
            "body": {
                "prompt": [
                    {"role": "user", "content": "Hello, can you help me with machine learning?"},
                    {"role": "assistant", "content": "Of course! I'd be happy to help you with machine learning."}
                ]
            }
        }
    ]
    
    log_lines = []
    for data in sample_data:
        message = f"[Log chat request] {json.dumps(data, ensure_ascii=False)}"
        log_json = {"message": message}
        log_line = f"{data['timestamp']} {json.dumps(log_json, ensure_ascii=False)}"
        log_lines.append(log_line)
    
    return log_lines


def test_shuffle_tokenizer():
    """æµ‹è¯•ç½®æ¢è¡¨è„±æ•åŠŸèƒ½"""
    print("ğŸ§ª å¼€å§‹æµ‹è¯•ç½®æ¢è¡¨è„±æ•å·¥å…·...")
    
    # åˆ›å»ºè„±æ•å¤„ç†å™¨
    desensitizer = SimpleTokenizerBasedDesensitizer(vocab_size=500)
    print("âœ… åˆ›å»ºè„±æ•å¤„ç†å™¨æˆåŠŸ")
    print(f"è¯æ±‡è¡¨å¤§å°: {desensitizer.vocab_size}")
    
    # æµ‹è¯•æ–‡æœ¬è„±æ•
    test_texts = [
        "ä½ å¥½ï¼Œä¸–ç•Œï¼",
        "Hello, World!",
        "Pythonç¼–ç¨‹è¯­è¨€",
        "Machine Learning"
    ]
    
    print("\nğŸ” æµ‹è¯•æ–‡æœ¬è„±æ•:")
    for text in test_texts:
        print(f"\nåŸå§‹æ–‡æœ¬: '{text}'")
        
        # ç¼–ç 
        original_ids = desensitizer.tokenizer.encode(text)
        print(f"åŸå§‹token IDs: {original_ids}")
        
        # è„±æ•
        desensitized_ids = desensitizer._desensitize_text(text)
        print(f"è„±æ•token IDs: {desensitized_ids}")
        
        # è§£ç éªŒè¯
        decoded_text = desensitizer.decode_tokens(desensitized_ids)
        print(f"è§£ç åæ–‡æœ¬: '{decoded_text}'")
        
        # éªŒè¯è§£ç åæ˜¯å¦ä¸åŸæ–‡ä¸€è‡´
        if decoded_text == text:
            print("âœ… è§£ç éªŒè¯æˆåŠŸ")
        else:
            print("âŒ è§£ç éªŒè¯å¤±è´¥")
    
    # æ˜¾ç¤ºä¸€äº›ç½®æ¢è¡¨æ ·ä¾‹
    print("\nğŸ—ï¸ ç½®æ¢è¡¨æ ·ä¾‹:")
    mapping_items = list(desensitizer.token_id_mapping.items())[:10]
    for original_id, mapped_id in mapping_items:
        original_char = desensitizer.tokenizer.id_to_char.get(original_id, "?")
        mapped_char = desensitizer.tokenizer.id_to_char.get(mapped_id, "?")
        print(f"  {original_id} ('{original_char}') -> {mapped_id} ('{mapped_char}')")
    
    # éªŒè¯ç½®æ¢è¡¨çš„å®Œæ•´æ€§
    print(f"\nğŸ“Š ç½®æ¢è¡¨ç»Ÿè®¡:")
    print(f"- æ˜ å°„æ•°é‡: {len(desensitizer.token_id_mapping)}")
    print(f"- è¯æ±‡è¡¨å¤§å°: {desensitizer.vocab_size}")
    print(f"- æ˜ å°„è¦†ç›–ç‡: {len(desensitizer.token_id_mapping)/desensitizer.vocab_size*100:.1f}%")
    
    # éªŒè¯ç½®æ¢è¡¨çš„ä¸€å¯¹ä¸€æ˜ å°„
    mapped_values = list(desensitizer.token_id_mapping.values())
    unique_values = set(mapped_values)
    if len(mapped_values) == len(unique_values):
        print("âœ… ç½®æ¢è¡¨æ˜¯ä¸€å¯¹ä¸€æ˜ å°„")
    else:
        print("âŒ ç½®æ¢è¡¨å­˜åœ¨é‡å¤æ˜ å°„")
    
    # éªŒè¯æ‰€æœ‰token IDéƒ½åœ¨è¯æ±‡è¡¨èŒƒå›´å†…
    all_in_range = all(0 <= v < desensitizer.vocab_size for v in mapped_values)
    if all_in_range:
        print("âœ… æ‰€æœ‰æ˜ å°„å€¼éƒ½åœ¨è¯æ±‡è¡¨èŒƒå›´å†…")
    else:
        print("âŒ å­˜åœ¨è¶…å‡ºè¯æ±‡è¡¨èŒƒå›´çš„æ˜ å°„å€¼")
    
    print("\nâœ… ç½®æ¢è¡¨è„±æ•å·¥å…·æµ‹è¯•å®Œæˆ!")


if __name__ == "__main__":
    test_shuffle_tokenizer() 