#!/usr/bin/env python3
"""
æµ‹è¯•åŸºäºTokenizerçš„è„±æ•å·¥å…·
"""

import json
import tempfile
import os
from datetime import datetime
from desensitize_dataset_tokenizer import TokenizerBasedDesensitizer

def create_sample_log_data():
    """åˆ›å»ºç¤ºä¾‹æ—¥å¿—æ•°æ®"""
    sample_data = [
        {
            "timestamp": "2024-01-01T10:00:01Z",
            "conversationId": "user_12345_session_abc",
            "body": {
                "prompt": [
                    {"role": "user", "content": "ä½ å¥½ï¼Œè¯·å¸®æˆ‘å†™ä¸€ä¸ªPythonç¨‹åº"},
                    {"role": "assistant", "content": "å¥½çš„ï¼Œæˆ‘å¯ä»¥å¸®æ‚¨å†™Pythonç¨‹åºã€‚è¯·å‘Šè¯‰æˆ‘æ‚¨å…·ä½“éœ€è¦ä»€ä¹ˆåŠŸèƒ½ï¼Ÿ"},
                    {"role": "user", "content": "æˆ‘éœ€è¦ä¸€ä¸ªè®¡ç®—æ–æ³¢é‚£å¥‘æ•°åˆ—çš„å‡½æ•°"}
                ]
            }
        },
        {
            "timestamp": "2024-01-01T10:00:02Z", 
            "conversationId": "user_67890_session_def",
            "body": {
                "prompt": [
                    {"role": "user", "content": "Hello, can you help me with machine learning?"},
                    {"role": "assistant", "content": "Of course! I'd be happy to help you with machine learning. What specific topic are you interested in?"}
                ]
            }
        },
        {
            "timestamp": "2024-01-01T10:00:03Z",
            "conversationId": "user_12345_session_abc",  # é‡å¤çš„ä¼šè¯ID
            "body": {
                "prompt": [
                    {"role": "user", "content": "è°¢è°¢ï¼Œè¿™ä¸ªå‡½æ•°çœ‹èµ·æ¥ä¸é”™"}
                ]
            }
        }
    ]
    
    log_lines = []
    for data in sample_data:
        # æ„é€ æ—¥å¿—è¡Œæ ¼å¼
        message = f"[Log chat request] {json.dumps(data, ensure_ascii=False)}"
        log_json = {"message": message}
        log_line = f"{data['timestamp']} {json.dumps(log_json, ensure_ascii=False)}"
        log_lines.append(log_line)
    
    return log_lines

def test_tokenizer_desensitizer():
    """æµ‹è¯•åŸºäºtokenizerçš„è„±æ•åŠŸèƒ½"""
    print("ğŸ§ª å¼€å§‹æµ‹è¯•åŸºäºTokenizerçš„è„±æ•å·¥å…·...")
    
    # åˆ›å»ºç¤ºä¾‹æ•°æ®
    sample_lines = create_sample_log_data()
    print(f"âœ… åˆ›å»ºäº† {len(sample_lines)} è¡Œç¤ºä¾‹æ•°æ®")
    
    # åˆ›å»ºä¸´æ—¶æ–‡ä»¶
    with tempfile.NamedTemporaryFile(mode='w', suffix='.log', delete=False, encoding='utf-8') as temp_input:
        for line in sample_lines:
            temp_input.write(line + '\n')
        temp_input_path = temp_input.name
    
    temp_output_path = temp_input_path.replace('.log', '_tokenized.json')
    temp_mapping_path = temp_input_path.replace('.log', '_mappings.json')
    
    try:
        print(f"ğŸ“ ä¸´æ—¶è¾“å…¥æ–‡ä»¶: {temp_input_path}")
        print(f"ğŸ“ ä¸´æ—¶è¾“å‡ºæ–‡ä»¶: {temp_output_path}")
        
        # åˆ›å»ºè„±æ•å¤„ç†å™¨ï¼ˆä½¿ç”¨gpt2ä½œä¸ºæµ‹è¯•tokenizerï¼Œé¿å…ä¸‹è½½å¤§æ¨¡å‹ï¼‰
        print("ğŸ”„ æ­£åœ¨åˆå§‹åŒ–tokenizer...")
        desensitizer = TokenizerBasedDesensitizer(model_name="gpt2")
        print("âœ… åˆ›å»ºè„±æ•å¤„ç†å™¨æˆåŠŸ")
        
        # å¤„ç†æ–‡ä»¶
        print("ğŸ”„ å¼€å§‹å¤„ç†æ–‡ä»¶...")
        desensitizer.process_file(temp_input_path, temp_output_path)
        print("âœ… æ–‡ä»¶å¤„ç†å®Œæˆ")
        
        # ä¿å­˜æ˜ å°„å…³ç³»
        desensitizer.save_mappings(temp_mapping_path)
        print("âœ… æ˜ å°„å…³ç³»ä¿å­˜å®Œæˆ")
        
        # è¯»å–å¹¶æ˜¾ç¤ºç»“æœ
        print("\nğŸ“Š å¤„ç†ç»“æœ:")
        print(f"- å¤„ç†è¡Œæ•°: {desensitizer.processed_lines}")
        print(f"- è·³è¿‡è¡Œæ•°: {desensitizer.skipped_lines}")
        print(f"- ä¼šè¯æ•°: {len(desensitizer.processed_conversations)}")
        print(f"- Token IDæ˜ å°„æ•°: {len(desensitizer.token_id_mapping)}")
        print(f"- Tokenizerè¯æ±‡è¡¨å¤§å°: {desensitizer.vocab_size}")
        
        # è¯»å–ç”Ÿæˆçš„JSONæ–‡ä»¶
        with open(temp_output_path, 'r', encoding='utf-8') as f:
            output_data = json.load(f)
        
        print(f"\nğŸ“‹ ç”Ÿæˆçš„JSONæ•°æ®åŒ…å« {len(output_data)} ä¸ªæ¡ç›®")
        
        # æ˜¾ç¤ºç¬¬ä¸€ä¸ªæ¡ç›®çš„è¯¦ç»†ä¿¡æ¯
        if output_data:
            first_item = output_data[0]
            print("\nğŸ” ç¬¬ä¸€ä¸ªæ¡ç›®è¯¦ç»†ä¿¡æ¯:")
            print(f"- æ—¶é—´æˆ³: {first_item['timestamp']}")
            print(f"- è„±æ•ä¼šè¯ID: {first_item['conversation_id']}")
            print(f"- æ¶ˆæ¯æ•°é‡: {len(first_item['messages'])}")
            
            # æ˜¾ç¤ºç¬¬ä¸€ä¸ªæ¶ˆæ¯çš„tokenåŒ–ç»“æœ
            if first_item['messages']:
                first_message = first_item['messages'][0]
                print(f"- ç¬¬ä¸€ä¸ªæ¶ˆæ¯è§’è‰²: {first_message.get('role', 'N/A')}")
                if 'content' in first_message:
                    token_ids = first_message['content']
                    print(f"- Token IDs (å‰10ä¸ª): {token_ids[:10]}")
                    print(f"- Token IDs æ€»æ•°: {len(token_ids)}")
                    
                    # å°è¯•è§£ç tokenä»¥éªŒè¯
                    decoded_text = desensitizer.decode_tokens(token_ids)
                    print(f"- è§£ç åçš„æ–‡æœ¬: {decoded_text[:50]}...")
        
        # æ˜¾ç¤ºä¸€äº›token IDæ˜ å°„æ ·ä¾‹
        print("\nğŸ—ï¸ Token IDæ˜ å°„æ ·ä¾‹:")
        mapping_items = list(desensitizer.token_id_mapping.items())[:10]
        for original_id, mapped_id in mapping_items:
            try:
                original_token = desensitizer.tokenizer.decode([original_id])
                mapped_token = desensitizer.tokenizer.decode([mapped_id])
                print(f"  {original_id} ('{original_token}') -> {mapped_id} ('{mapped_token}')")
            except:
                print(f"  {original_id} -> {mapped_id}")
        
        # æ˜¾ç¤ºä¼šè¯IDæ˜ å°„
        print("\nğŸ†” ä¼šè¯IDæ˜ å°„:")
        for original, mapped in desensitizer.conversation_id_mapping.items():
            print(f"  '{original}' -> '{mapped}'")
        
        print("\nâœ… æµ‹è¯•å®Œæˆ! åŸºäºTokenizerçš„è„±æ•å·¥å…·å·¥ä½œæ­£å¸¸ã€‚")
        
        # éªŒè¯JSONæ ¼å¼
        print("\nğŸ” éªŒè¯JSONæ ¼å¼...")
        validate_json_format(temp_output_path)
        
    finally:
        # æ¸…ç†ä¸´æ—¶æ–‡ä»¶
        for path in [temp_input_path, temp_output_path, temp_mapping_path]:
            if os.path.exists(path):
                os.unlink(path)
        print("ğŸ—‘ï¸ ä¸´æ—¶æ–‡ä»¶æ¸…ç†å®Œæˆ")

def validate_json_format(json_file):
    """éªŒè¯ç”Ÿæˆçš„JSONæ–‡ä»¶æ ¼å¼"""
    with open(json_file, 'r', encoding='utf-8') as f:
        data = json.load(f)
    
    assert isinstance(data, list), "é¡¶å±‚åº”è¯¥æ˜¯åˆ—è¡¨"
    
    for i, item in enumerate(data):
        assert isinstance(item, dict), f"ç¬¬{i}ä¸ªæ¡ç›®åº”è¯¥æ˜¯å­—å…¸"
        assert 'timestamp' in item, f"ç¬¬{i}ä¸ªæ¡ç›®ç¼ºå°‘timestampå­—æ®µ"
        assert 'conversation_id' in item, f"ç¬¬{i}ä¸ªæ¡ç›®ç¼ºå°‘conversation_idå­—æ®µ"
        assert 'messages' in item, f"ç¬¬{i}ä¸ªæ¡ç›®ç¼ºå°‘messageså­—æ®µ"
        assert 'metadata' in item, f"ç¬¬{i}ä¸ªæ¡ç›®ç¼ºå°‘metadataå­—æ®µ"
        
        # éªŒè¯messagesæ ¼å¼
        messages = item['messages']
        assert isinstance(messages, list), f"ç¬¬{i}ä¸ªæ¡ç›®çš„messagesåº”è¯¥æ˜¯åˆ—è¡¨"
        
        for j, msg in enumerate(messages):
            if isinstance(msg, dict):
                if 'content' in msg:
                    content = msg['content']
                    assert isinstance(content, list), f"ç¬¬{i}ä¸ªæ¡ç›®ç¬¬{j}ä¸ªæ¶ˆæ¯çš„contentåº”è¯¥æ˜¯token IDåˆ—è¡¨"
                    assert all(isinstance(token_id, int) for token_id in content), f"ç¬¬{i}ä¸ªæ¡ç›®ç¬¬{j}ä¸ªæ¶ˆæ¯çš„contentåº”è¯¥åŒ…å«æ•´æ•°token IDs"
    
    print(f"âœ… JSONæ ¼å¼éªŒè¯é€šè¿‡: {len(data)} ä¸ªæ¡ç›®")

if __name__ == "__main__":
    test_tokenizer_desensitizer() 